# -*- coding: utf-8 -*-
"""Bidirectional_LSTM_Glove.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M-NP2Dkmvg4qn9peltGlD4cxUR8qEHjw
"""

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, Embedding, CuDNNLSTM, Bidirectional
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.callbacks import EarlyStopping
from keras.models import load_model
from sklearn.metrics import roc_auc_score
from tqdm import tqdm
import pandas as pd
import numpy as np

df = pd.read_csv(r'/content/drive/My Drive/Quara Spam filter/train.csv')

df_1 = df[df['target']==1]
df_1.reset_index(inplace=True)
df_0 = df[df['target']==0]
df_00 = df_0[0:df_1.shape[0]]
df_00.reset_index(inplace=True)
df_10 = pd.concat([df_1,df_00],axis=0)

#Train/Test Split
df_train,df_test=train_test_split(df_10,test_size=0.2,random_state=2)

!unzip '/content/drive/My Drive/glove.840B.300d.zip'

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

token = Tokenizer()
token.fit_on_texts(df_train['question_text'])
seq = token.texts_to_sequences(df_train['question_text'])

pad_seq = pad_sequences(seq,maxlen=300)

vocab_size = len(token.word_index)+1

embedding_vector = {}
f = open('/content/glove.840B.300d.txt')
for line in tqdm(f):
    value = line.split(' ')
    word = value[0]
    coef = np.array(value[1:],dtype = 'float32')
    embedding_vector[word] = coef

embedding_matrix = np.zeros((vocab_size,300))
for word,i in tqdm(token.word_index.items()):
    embedding_value = embedding_vector.get(word)
    if embedding_value is not None:
        embedding_matrix[i] = embedding_value

model = Sequential()
model.add(Embedding(vocab_size,300,weights = [embedding_matrix],input_length=300,trainable = False))
model.add(Bidirectional(CuDNNLSTM(126)))
model.add(Dense(64,activation = 'relu'))
model.add(Dense(128,activation = 'relu'))
model.add(Dropout(0.3))
model.add(Dense(1,activation = 'sigmoid'))
model.compile(optimizer='adam',loss='binary_crossentropy',metrics = ['accuracy'])


outputFolder = '/content/drive/My Drive/Quora_glove_model/'
filepath=outputFolder+"/weights-{epoch:02d}-{val_accuracy:.4f}.h5"
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', 
                             verbose=1, 
                             save_best_only=True,
                             save_weights_only=False, 
                             mode='auto')

earlystop = EarlyStopping(monitor='val_loss', 
                          min_delta=0.01, patience=10,
                          verbose=1, mode='auto')

model.fit(pad_seq, df_train['target'],epochs = 50,batch_size=256,validation_split=0.2,callbacks=[earlystop,checkpoint])


model = load_model('/content/drive/My Drive/Quora_glove_model/weights-04-0.9022.h5')

token.fit_on_texts(df_test['question_text'])
sequences = token.texts_to_sequences(df_test['question_text'])

test_pad_seq = pad_sequences(sequences,maxlen=300)

predictions=model.predict(test_pad_seq)

roc_auc_score(np.asarray(df_test['target']),predictions)

